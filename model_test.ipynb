{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize variables\n",
    "documents = []\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the directory containing the PDF files\n",
    "pdf_directory = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문을 벡터 db에 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf를 사용해서 pdf(논문)을 모두 로드\n",
    "pdf_files = glob(os.path.join(pdf_directory, '*.pdf'))\n",
    "\n",
    "# Load all PDF files using PyPDFLoader\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pdf_documents = loader.load()\n",
    "    documents.extend(pdf_documents)\n",
    "    \n",
    "# 텍스트는 RecursiveCharacterTextSplitter를 사용하여 분할\n",
    "chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = chunk_splitter.split_documents(documents)\n",
    "\n",
    "# embeddings은 OpenAI의 임베딩을 사용\n",
    "# vectordb는 chromadb사용함\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize the OpenAIEmbeddings and ChatOpenAI objects\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=chat_model, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-4o\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "# 환경 변수 불러오기\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize variables\n",
    "documents = []\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the directory containing the PDF files\n",
    "pdf_directory = './data'\n",
    "\n",
    "# Load all PDF files using PyPDFLoader\n",
    "pdf_files = glob(os.path.join(pdf_directory, '*.pdf'))\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pdf_documents = loader.load()\n",
    "    documents.extend(pdf_documents)\n",
    "\n",
    "# 텍스트를 RecursiveCharacterTextSplitter를 사용하여 분할\n",
    "chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = chunk_splitter.split_documents(documents)\n",
    "\n",
    "# Embedding 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Chroma를 이용해 Vector DB 생성\n",
    "vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# ChatOpenAI 모델 초기화\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 시스템 프롬프트 설정\n",
    "SYS_PROMPT = \"\"\"\n",
    "너는 사용자의 인적정보, KGLS 질문과 사용자답변, 사용자가 입력한 주관식 답변 그리고 논문들의 외로움에 관한 연구를 바탕으로 이와 관련된 외로움을 가지고 있는지 설정해주고 그 사용자에게 적절한 대화 상대가 되기 위한 프롬프트를 출력해줘.\n",
    "\"\"\"\n",
    "# 분석 요청 입력 프롬프트 설정\n",
    "input_prompt = \"\"\"\n",
    "이 질문과 답변을 사용자의 외로움을 파악하기 위함이야. 논문에서 참고해서 사용자가 어떤 외로움을 느끼는건지, 어떤 대화상대가 필요할지 알아보고, 사용자가 경험한 외로움이 뭔지와 적절한 대화하기 위한 프롬프트를 출력해줘.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 예시 입력\n",
    "example_input = {\n",
    "\n",
    "    \"인적정보\": \"홍길동, 35세, 남성\",\n",
    "\n",
    "    \"KGLS질문\": \"최근에 외로움을 느꼈나요?\",\n",
    "\n",
    "    \"주관식 질문\": \"외로움을 느낀 이유가 무엇인가요?\"\n",
    "\n",
    "}\n",
    "input_prompt += example_input\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        (\"human\", \"{input_prompt}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiservice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
